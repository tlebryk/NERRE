GPT-3 Doping-JSON
------------------
Scoring outputs using 
        test file: /Users/ardunn/alex/lbl/projects/matscholar/ardunn_text_experiments/nerre_official_provenance_repository/doping/data/test.json
        pred file: data/inference_decoded_json.json
basemats: prec=0.8876404494382022, recall=0.8876404494382022, f1=0.8876404494382022
dopants: prec=0.7792207792207793, recall=0.7142857142857143, f1=0.7453416149068323
triplets: prec=0.7722772277227723, recall=0.6842105263157895, f1=0.7255813953488371
      metric         entity     score
0  precision       basemats  0.887640
1     recall       basemats  0.887640
2         f1       basemats  0.887640
3  precision        dopants  0.779221
4     recall        dopants  0.714286
5         f1        dopants  0.745342
6  precision  link triplets  0.772277
7     recall  link triplets  0.684211
8         f1  link triplets  0.725581
Total sequences was: 77
Frac. Sequences parsable:  0.987012987012987
Avg sequence similarity:  0.9657836826528741
Frac. of sequences exactly correct:  0.6493506493506493
Support was:  {'ents': {'basemats': 60, 'dopants': 76},
 'links_ents': 72,
 'links_words': 114,
 'words': {'basemats': 111, 'dopants': 110}}



GPT-3 Doping-English
--------------------
Scoring outputs using 
        test file: /Users/ardunn/alex/lbl/projects/matscholar/ardunn_text_experiments/nerre_official_provenance_repository/doping/data/test.json
        pred file: data/inference_decoded_eng.json
basemats: prec=0.8888888888888888, recall=0.898876404494382, f1=0.8938547486033519
dopants: prec=0.8441558441558441, recall=0.7738095238095238, f1=0.8074534161490683
triplets: prec=0.8037383177570093, recall=0.7543859649122807, f1=0.7782805429864253
      metric         entity     score
0  precision       basemats  0.888889
1     recall       basemats  0.898876
2         f1       basemats  0.893855
3  precision        dopants  0.844156
4     recall        dopants  0.773810
5         f1        dopants  0.807453
6  precision  link triplets  0.803738
7     recall  link triplets  0.754386
8         f1  link triplets  0.778281
Total sequences was: 77
Frac. Sequences parsable:  0.987012987012987
Avg sequence similarity:  0.9446225963140241
Frac. of sequences exactly correct:  0.5974025974025974
Support was:  {'ents': {'basemats': 60, 'dopants': 76},
 'links_ents': 72,
 'links_words': 114,
 'words': {'basemats': 111, 'dopants': 110}}




GPT-3 Doping-EngExtra
---------------------
Scoring outputs using
        test file: /Users/ardunn/alex/lbl/projects/matscholar/ardunn_text_experiments/nerre_official_provenance_repository/doping/data/test.json
        pred file: data/inference_decoded_engextra.json
basemats: prec=0.8817204301075269, recall=0.9213483146067416, f1=0.9010989010989011
dopants: prec=0.8554216867469879, recall=0.8452380952380952, f1=0.8502994011976048
results: prec=0.9090909090909091, recall=0.5555555555555556, f1=0.6896551724137931
doping_modifiers: prec=0.13636363636363635, recall=0.42857142857142855, f1=0.20689655172413793
triplets: prec=0.8198198198198198, recall=0.7982456140350878, f1=0.8088888888888889
       metric            entity     score
0   precision          basemats  0.881720
1      recall          basemats  0.921348
2          f1          basemats  0.901099
3   precision           dopants  0.855422
4      recall           dopants  0.845238
5          f1           dopants  0.850299
6   precision           results  0.909091
7      recall           results  0.555556
8          f1           results  0.689655
9   precision  doping_modifiers  0.136364
10     recall  doping_modifiers  0.428571
11         f1  doping_modifiers  0.206897
12  precision     link triplets  0.819820
13     recall     link triplets  0.798246
14         f1     link triplets  0.808889
Total sequences was: 77
Frac. Sequences parsable:  0.987012987012987
Avg sequence similarity:  0.9387300801304119
Frac. of sequences exactly correct:  0.5844155844155844
Support was:  {'ents': {'basemats': 60, 'dopants': 76, 'doping_modifiers': 7, 'results': 12},
 'links_ents': 72,
 'links_words': 114,
 'words': {'basemats': 111,
           'dopants': 110,
           'doping_modifiers': 77,
           'results': 103}}



Llama 2 Doping-JSON
---------------------
Scoring outputs using 
        test file: /Users/ardunn/alex/lbl/projects/matscholar/ardunn_text_experiments/nerre_official_provenance_repository/doping/data/test.json
        pred file: data/llama2/inference_decoded_json.json
basemats: prec=0.8709677419354839, recall=0.9101123595505618, f1=0.8901098901098901
dopants: prec=0.872093023255814, recall=0.8928571428571429, f1=0.8823529411764706
triplets: prec=0.8363636363636363, recall=0.8070175438596491, f1=0.8214285714285714
      metric         entity     score
0  precision       basemats  0.870968
1     recall       basemats  0.910112
2         f1       basemats  0.890110
3  precision        dopants  0.872093
4     recall        dopants  0.892857
5         f1        dopants  0.882353
6  precision  link triplets  0.836364
7     recall  link triplets  0.807018
8         f1  link triplets  0.821429
Total sequences was: 77
Frac. Sequences parsable:  1.0
Avg sequence similarity:  0.9754603518959042
Frac. of sequences exactly correct:  0.7142857142857143
Support was:  {'ents': {'basemats': 60, 'dopants': 76},
 'links_ents': 72,
 'links_words': 114,
 'words': {'basemats': 111, 'dopants': 110}}




Llama 2 Doping-English
-------------------------
Scoring outputs using 
        test file: /Users/ardunn/alex/lbl/projects/matscholar/ardunn_text_experiments/nerre_official_provenance_repository/doping/data/test.json
        pred file: data/llama2/inference_decoded_eng.json
basemats: prec=0.8383838383838383, recall=0.9325842696629213, f1=0.8829787234042553
dopants: prec=0.8372093023255814, recall=0.8571428571428571, f1=0.8470588235294118
triplets: prec=0.7868852459016393, recall=0.8421052631578947, f1=0.8135593220338982
      metric         entity     score
0  precision       basemats  0.838384
1     recall       basemats  0.932584
2         f1       basemats  0.882979
3  precision        dopants  0.837209
4     recall        dopants  0.857143
5         f1        dopants  0.847059
6  precision  link triplets  0.786885
7     recall  link triplets  0.842105
8         f1  link triplets  0.813559
Total sequences was: 77
Frac. Sequences parsable:  1.0
Avg sequence similarity:  0.9461784105659841
Frac. of sequences exactly correct:  0.6493506493506493
Support was:  {'ents': {'basemats': 60, 'dopants': 76},
 'links_ents': 72,
 'links_words': 114,
 'words': {'basemats': 111, 'dopants': 110}}


Llama 2 DopingExtra-English
--------------------------
Scoring outputs using 
        test file: /Users/ardunn/alex/lbl/projects/matscholar/ardunn_text_experiments/nerre_official_provenance_repository/doping/data/test.json
        pred file: data/llama2/inference_decoded_engextra.json
basemats: prec=0.8163265306122449, recall=0.898876404494382, f1=0.8556149732620321
dopants: prec=0.7010309278350515, recall=0.8095238095238095, f1=0.7513812154696132
results: prec=1.0, recall=0.3888888888888889, f1=0.56
doping_modifiers: prec=0.16666666666666666, recall=0.42857142857142855, f1=0.24
triplets: prec=0.6940298507462687, recall=0.8157894736842105, f1=0.75
       metric            entity     score
0   precision          basemats  0.816327
1      recall          basemats  0.898876
2          f1          basemats  0.855615
3   precision           dopants  0.701031
4      recall           dopants  0.809524
5          f1           dopants  0.751381
6   precision           results  1.000000
7      recall           results  0.388889
8          f1           results  0.560000
9   precision  doping_modifiers  0.166667
10     recall  doping_modifiers  0.428571
11         f1  doping_modifiers  0.240000
12  precision     link triplets  0.694030
13     recall     link triplets  0.815789
14         f1     link triplets  0.750000
Total sequences was: 77
Frac. Sequences parsable:  1.0
Avg sequence similarity:  0.912043625195695
Frac. of sequences exactly correct:  0.5584415584415584
Support was:  {'ents': {'basemats': 60, 'dopants': 76, 'doping_modifiers': 7, 'results': 12},
 'links_ents': 72,
 'links_words': 114,
 'words': {'basemats': 111,
           'dopants': 110,
           'doping_modifiers': 77,
           'results': 103}}



Seq2Rel
--------
Scoring outputs using
        test file: /Users/ardunn/alex/lbl/projects/matscholar/ardunn_text_experiments/nerre_official_provenance_repository/doping/data/test.json
        pred file: data/seq2rel/seq2rel_doping_evaluation_all_lowercase.json
basemats: prec=0.43103448275862066, recall=0.8426966292134831, f1=0.5703422053231939
dopants: prec=0.5238095238095238, recall=0.6547619047619048, f1=0.582010582010582
triplets: prec=0.42073170731707316, recall=0.6052631578947368, f1=0.49640287769784175
      metric         entity     score
0  precision       basemats  0.431034
1     recall       basemats  0.842697
2         f1       basemats  0.570342
3  precision        dopants  0.523810
4     recall        dopants  0.654762
5         f1        dopants  0.582011
6  precision  link triplets  0.420732
7     recall  link triplets  0.605263
8         f1  link triplets  0.496403
Total sequences was: 77
Frac. Sequences parsable:  1.0
Avg sequence similarity:  0.9140134292492434
Frac. of sequences exactly correct:  0.33766233766233766
Support was:  {'ents': {'basemats': 60, 'dopants': 76},
 'links_ents': 72,
 'links_words': 114,
 'words': {'basemats': 111, 'dopants': 110}}



MatBERT+Proximity
--------------------
Scoring outputs using
        test file: /Users/ardunn/alex/lbl/projects/matscholar/ardunn_text_experiments/nerre_official_provenance_repository/doping/data/test.json
        pred file: data/matbert-proximity/matbert_predictions_correct_format.json
basemats: prec=0.620253164556962, recall=0.550561797752809, f1=0.5833333333333334
dopants: prec=0.5384615384615384, recall=0.5, f1=0.5185185185185185
triplets: prec=0.3770491803278688, recall=0.40350877192982454, f1=0.3898305084745763
      metric         entity     score
0  precision       basemats  0.620253
1     recall       basemats  0.550562
2         f1       basemats  0.583333
3  precision        dopants  0.538462
4     recall        dopants  0.500000
5         f1        dopants  0.518519
6  precision  link triplets  0.377049
7     recall  link triplets  0.403509
8         f1  link triplets  0.389831
Total sequences was: 77
Frac. Sequences parsable:  1.0
Avg sequence similarity:  0.6710716880103103
Frac. of sequences exactly correct:  0.0
Support was:  {'ents': {'basemats': 60, 'dopants': 76},
 'links_ents': 72,
 'links_words': 114,
 'words': {'basemats': 111, 'dopants': 110}}
